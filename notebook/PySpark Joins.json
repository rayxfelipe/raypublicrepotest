{
	"name": "PySpark Joins",
	"properties": {
		"folder": {
			"name": "BHDW"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a9b93353-09a1-4e92-802e-64cb4b90786e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/499bc654-f84c-46c2-952c-b30be508f78c/resourceGroups/rg-synapse-demo-001/providers/Microsoft.Synapse/workspaces/synw-demoworkspace-001/bigDataPools/sparkpool1",
				"name": "sparkpool1",
				"type": "Spark",
				"endpoint": "https://synw-demoworkspace-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"Performing DataFrame joins in PySpark is similar to SQL joins. Hereâ€™s a quick guide on how to do it:\r\n",
					"\r\n",
					"### Basic Syntax\r\n",
					"The `join()` method is used to combine two DataFrames. The basic syntax is:"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					}
				},
				"source": [
					"```python\n",
					"df1.join(df2, on, how)\n",
					"```\n",
					"- **df1**: The first DataFrame.\n",
					"- **df2**: The second DataFrame.\n",
					"- **on**: The column(s) to join on.\n",
					"- **how**: The type of join (e.g., 'inner', 'left', 'right', 'outer')."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Example\r\n",
					"Let's say you have two DataFrames, `empDF` and `deptDF`:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder.appName(\"example\").getOrCreate()\r\n",
					"\r\n",
					"# Sample data\r\n",
					"empData = [(1, \"Smith\", 10), (2, \"Rose\", 20), (3, \"Williams\", 10), (4, \"Jones\", 30)]\r\n",
					"empColumns = [\"emp_id\", \"name\", \"emp_dept_id\"]\r\n",
					"empDF = spark.createDataFrame(empData, empColumns)\r\n",
					"\r\n",
					"deptData = [(\"Finance\", 10), (\"Marketing\", 20), (\"Sales\", 30), (\"IT\", 40)]\r\n",
					"deptColumns = [\"dept_name\", \"dept_id\"]\r\n",
					"deptDF = spark.createDataFrame(deptData, deptColumns)\r\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Inner Join\r\n",
					"An inner join returns rows that have matching values in both DataFrames:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"inner_join_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"inner\")\r\n",
					"inner_join_df.show()\r\n",
					"```\r\n",
					"\r\n",
					"### Left Join\r\n",
					"A left join returns all rows from the left DataFrame and matched rows from the right DataFrame:"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"left_join_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"left\")\r\n",
					"left_join_df.show()\r\n",
					"```"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Right Join\r\n",
					"A right join returns all rows from the right DataFrame and matched rows from the left DataFrame:"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"right_join_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"right\")\r\n",
					"right_join_df.show()\r\n",
					"```"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Full Outer Join\r\n",
					"A full outer join returns all rows when there is a match in either DataFrame:"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"outer_join_df = empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"outer\")\r\n",
					"outer_join_df.show()\r\n",
					"```"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Dropping Duplicate Columns\r\n",
					"If you want to drop duplicate columns after a join:"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"joined_df = empDF.join(deptDF, \"emp_dept_id\")\r\n",
					"joined_df.show()\r\n",
					"```"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Using SQL for Joins\r\n",
					"You can also use SQL queries to perform joins:"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"```python\r\n",
					"empDF.createOrReplaceTempView(\"EMP\")\r\n",
					"deptDF.createOrReplaceTempView(\"DEPT\")\r\n",
					"sql_join_df = spark.sql(\"SELECT * FROM EMP e JOIN DEPT d ON e.emp_dept_id = d.dept_id\")\r\n",
					"sql_join_df.show()\r\n",
					"```"
				],
				"execution_count": null
			}
		]
	}
}